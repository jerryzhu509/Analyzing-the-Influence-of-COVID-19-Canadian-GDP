# -*- coding: utf-8 -*-
"""CSC110 Fall 2021 Project, Part 2: Building Model

Notes:
===============================
This program builds the GDP prediction model.
reading_data.ipynb is automatically generated by Colaboratory.
Original file is located at
https://colab.research.google.com/drive/1RPnv3EVqAXJRHo2r26D__FvmhoDQgtay

Copyright and Usage Information
===============================

This file is provided solely for the personal and private use of students
taking CSC110 at the University of Toronto St. George campus. All forms of
distribution of this code, whether as given or with any changes, are
expressly prohibited. For more information on copyright for CSC110 materials,
please consult our Course Syllabus.

This file is Copyright (c) 2021 Jerry Zhu, Jack Sun, Nicholas Au
"""

import math
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.exceptions import ConvergenceWarning
from sklearn.linear_model import ElasticNet
from sklearn.svm import LinearSVR
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import plotly.express as px
from reading_data import loading_data

pd.options.mode.chained_assignment = None  # default='warn'
warnings.simplefilter("ignore", category=ConvergenceWarning)


# Section 1 - Processing the DataFrame
def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Return a new Dataframe after processing df made from make_dataframe().

    Preconditions:
    - 'gdp' in df.columns
    - 'unemployment_rate' in df.columns
    - 'composite_index' in df.columns
    - 'exchange_rate' in df.columns
    - 'bond_yield' in df.columns
    - 'inflation_rate' in df.columns
    - 'overnight_rate' in df.columns

    >>> df = process_dataframe(make_dataframe(loading_data()))
    >>> {'dates', 'unemployment_rate', 'balance_of_payment', 'gdp', 'population', \
       'composite_index', 'exchange_rate', 'bond_yield', 'inflation_rate', \
       'overnight_rate', 'gdp_growth'} == set(df.columns)
    True

    >>> df = process_dataframe(make_dataframe(loading_data()))
    >>> all({num_nulls == 0 for num_nulls in df.isna().sum()})
    True
    """
    # Call this method once in the function body to avoid the python-ta error of unused imports
    # Note, this function from reading_data.py is used for doctest purposes only in model.py
    loading_data()

    # Save dates
    dates = []
    for x in df['gdp']:
        if x is not None:
            dates.append(x[0])
        else:
            dates.append(None)

    # Strip date from indicator tuples
    for name in df:
        temp = []
        for x in df[name]:
            if x is not None:
                temp.append(x[1])
            else:
                temp.append(None)
        df[name] = temp

    # Add in date as a column
    df.insert(0, 'dates', dates)

    # Add in recent values
    # https://ycharts.com/indicators/canada_unemployment_rate
    df['unemployment_rate'][51] = 9.0
    df['unemployment_rate'][52] = 9.4
    df['unemployment_rate'][53] = 8.1

    # https://tradingeconomics.com/canada/stock-market
    df['composite_index'][53] = 18990.30

    # https://tradingeconomics.com/usdcad:cur
    df['exchange_rate'][53] = 1.2545

    # https://tradingeconomics.com/canada/government-bond-yield
    df['bond_yield'][53] = 1.5

    # https://tradingeconomics.com/canada/inflation-cpi
    df['inflation_rate'][53] = 3.4

    # https://tradingeconomics.com/canada/interest-rate
    df['overnight_rate'][53] = 0.25

    # Compute gdp_growth and add it as a column
    gdp_growth = [0]
    for i in range(1, len(df['gdp'])):
        gdp_growth.append((df['gdp'][i] - df['gdp'][i - 1]) / df['gdp'][i - 1] * 100)
    df['gdp_growth'] = gdp_growth

    return df


def make_dataframe(data: dict[str, list[tuple]]) -> pd.DataFrame:
    """Return a pandas DataFrame object from the data read by loading_data()."""
    return pd.DataFrame.from_dict(data, orient='index').T


# Section 2 - Prepare Data for Machine Learning
def transform_data(data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],
                   transformations: tuple[StandardScaler, PCA]) -> \
        tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Return the training and testing data after applying transformations to data."""
    x_train, x_test, y_train, y_test = data[0], data[1], data[2], data[3]

    # Get transformations
    scaler, pca = transformations[0], transformations[1]

    # Return transformed data.
    return (pca.transform(scaler.transform(x_train)), pca.transform(scaler.transform(x_test)),
            y_train, y_test)


def split_data(data: tuple[np.ndarray, np.ndarray]) -> \
        tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Return, as a 4-tuple from df, the training and test sets of x and y as np.ndarray objects."""
    # Separate the training and testing set
    x_train, x_test, y_train, y_test = train_test_split(
        data[0], data[1], test_size=0.45, random_state=123)

    return (x_train, x_test, y_train, y_test)


def separate_indepedent_dependent(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    """Return, as a 2-tuple from df, the array of independent variables
    and the dependent variables as numpy.ndarray objects.

    Preconditions:
    - all({num_nulls == 0 for num_nulls in df.isna().sum()})
    - 'dates' in df.columns
    - 'gdp' in df.columns
    - 'gdp_growth' in df.columns
    """

    # We build the model based on data from 2012-2017
    df_2012_2017 = df.iloc[0:39]

    # Getting the X array (the array of independent variables)
    x = df_2012_2017.drop(['gdp', 'dates', 'gdp_growth'], axis=1).values

    # Gdp is the dependant variable. It is generally not necessary
    # to transform the dependent variable
    y = df_2012_2017["gdp"].values

    return x, y


def get_data_transformations(data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]) -> \
        tuple[StandardScaler, PCA]:
    """Return StandardScaler and PCA after fitting it to the training data. They are to scale and
    reduce the dimensionality of the arrays."""
    x_train = data[0]

    # Fit StandardScaler on x_train
    scaler = StandardScaler()
    scaler.fit(x_train)
    scaled_x_train = scaler.transform(x_train)

    # Reduce dimensionality of data by 1
    pca = PCA(n_components=7)
    pca.fit(scaled_x_train)
    return (scaler, pca)


# Section 3 - Machine Learning
def make_predictions(df: pd.DataFrame, transformations: tuple[StandardScaler, PCA],
                     predictor: ElasticNet) -> np.ndarray:
    """Return predicted_gdp as type np.ndarray on data transformed using transformations from df.
    This is done with predictor."""
    scaler, pca = transformations[0], transformations[1]

    # Make Predictions
    y_pred = predictor.predict(
        pca.transform(scaler.transform(df.drop(['gdp', 'dates', 'gdp_growth'], axis=1).values)))

    return y_pred


def select_best_model(data: tuple[np.ndarray, np.ndarray],
                      train_test_data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],
                      transformations: tuple[StandardScaler, PCA]) -> ElasticNet:
    """Return the most accurate machine learning model. We perform 5-fold cross validation with
    'Elastic Net', 'Linear SVR', 'Random Forest', 'Gradient Boosting Regressor', and 'Extra Trees',
    returning the most accurate one."""
    # Get x_train, y_train
    x_train, y_train = train_test_data[0], train_test_data[2]

    # We fit a number of machine learning models.

    # ElasticNet Regression
    enr = ElasticNet()
    enr.fit(x_train, np.ravel(y_train))

    # Linear SupportVectorRegressor
    linearsvm = LinearSVR(max_iter=2000)
    linearsvm.fit(x_train, np.ravel(y_train))

    # RandomForestRegressor
    random_forest = RandomForestRegressor()
    random_forest.fit(x_train, np.ravel(y_train))

    # GradientBoostingRegressor
    gbes = GradientBoostingRegressor()
    gbes.fit(x_train, np.ravel(y_train))

    # ExtraTreesRegressor
    extra_trees = ExtraTreesRegressor()
    extra_trees.fit(x_train, np.ravel(y_train))

    # We apply 5 fold Cross Validation to determine the best performing model.
    np.random.seed(123)
    rmse = {}
    for regressor in [enr, linearsvm, random_forest, gbes, extra_trees]:
        cv_scores = cross_val_score(regressor, transformations[1].
                                    transform(transformations[0].transform(data[0])), data[1], cv=5,
                                    scoring='neg_mean_squared_error')
        rmse[regressor] = round(math.sqrt(cv_scores.mean() * -1), 3)

    # Return the most accurate regressor. Note, ElasticNet performs the best.
    return min(rmse, key=rmse.get)


def tune_model(train_test_data: tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],
               predictor: ElasticNet) -> ElasticNet:
    """Return fine-tuned predictor after hyper-parameter tuning."""
    # Assign variables from inputs
    x_train, y_train = train_test_data[0], train_test_data[2]

    # We now perform hyperparameter tuning on ElasticNet to optimize performance.

    # Grid of parameters
    grid_search = {"tol": [0.001, 0.1, 0.5, 1, 5, 10, 100, 1000],
                   "alpha": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 100],
                   "l1_ratio": np.arange(0.0, 1.0, 0.05)}

    # Elastic Net
    best_predictor = GridSearchCV(estimator=predictor, param_grid=grid_search,
                                  cv=5, scoring='neg_mean_squared_error')
    best_predictor.fit(x_train, y_train)
    best_predictor.best_estimator_.fit(x_train, np.ravel(y_train))
    return best_predictor.best_estimator_


# Section 4 - Visualization
def plot_graphs(df: pd.DataFrame) -> None:
    """Plot useful graphs based on attributes in df. Returns none.

    Preconditions:
    - all({num_nulls == 0 for num_nulls in df.isna().sum()})
    """
    # Plot correlation heatmap
    fig = px.imshow(df.corr())
    fig.show()

    # Plot histograms
    df.hist(figsize=(10, 7))
    plt.tight_layout()


def visualize_predictions(df: pd.DataFrame, predictions: np.ndarray) -> None:
    """Plot graphs visualizing the predictions values against the actual values in df.
    Returns none.

    Preconditions:
    - all({num_nulls == 0 for num_nulls in df.isna().sum()})
    - 'dates' in df.columns
    - 'gdp' in df.columns
    - 'gdp_growth' in df.columns
    """
    df['predicted_gdp'] = predictions

    # Visualize gdp vs predicted_gdp
    plt.figure(2)
    plt.title("gdp vs predicted_gdp")
    plt.plot(df['dates'], df['gdp'], label='actual')
    plt.plot(df['dates'], df['predicted_gdp'], label='predicted')
    plt.legend(loc="best")
    plt.xlabel("Date")
    plt.ylabel("GDP in 2012 CAD, Millions")

    # Compute predicted_gdp growth
    predicted_gdp_growth = [0]
    for i in range(1, len(df['gdp'])):
        predicted_gdp_growth.append((df['predicted_gdp'][i] - df['predicted_gdp'][i - 1])
                                    / df['predicted_gdp'][i - 1] * 100)
    df['predicted_gdp_growth'] = predicted_gdp_growth

    # Print RMSE
    print("RMSE of ElasticNet on GDP for 2008-2017 is: " + str(np.sqrt(mean_squared_error(
        df['predicted_gdp'].iloc[0:40], df['gdp'].iloc[0:40]))))

    print("RMSE of ElasticNet on GDP for 2018-2019 is: " + str(np.sqrt(mean_squared_error(
        df['predicted_gdp'].iloc[40:48], df['gdp'].iloc[40:48]))))

    print("RMSE of ElasticNet on GDP for 2020-2021 is: " + str(np.sqrt(mean_squared_error(
        df['predicted_gdp'].iloc[48:54], df['gdp'].iloc[48:54]))))

    print("RMSE of predicted_gdp_growth (%) of ElasticNet for 2008-2017 is:",
          str(np.sqrt(mean_squared_error(df['predicted_gdp_growth'].iloc[0:40],
                                         df['gdp_growth'].iloc[0:40]))))

    print("RMSE of predicted_gdp_growth (%) of ElasticNet for 2018-2019 is:",
          str(np.sqrt(mean_squared_error(df['predicted_gdp_growth'].iloc[40:48],
                                         df['gdp_growth'].iloc[40:48]))))

    print("RMSE of predicted_gdp_growth (%) of ElasticNet for 2020-2021 is:",
          str(np.sqrt(mean_squared_error(df['predicted_gdp_growth'].iloc[48:54],
                                         df['gdp_growth'].iloc[48:54]))))

    # Visualize gdp_growth vs predicted_gdp_growth
    plt.figure(3)
    plt.title("gdp_growth vs predicted_gdp_growth")
    plt.plot(df['dates'], df['gdp_growth'], label='actual')
    plt.plot(df['dates'], df['predicted_gdp_growth'], label='predicted')
    plt.legend(loc="best")
    plt.xlabel("Date")
    plt.ylabel("GDP Growth (%)")


if __name__ == '__main__':
    # When you are ready to check your work with python_ta, uncomment the following lines.
    # (Delete the "#" and space before each line.)
    # IMPORTANT: keep this code indented inside the "if __name__ == '__main__'" block
    # Leave this code uncommented when you submit your files.

    import doctest

    doctest.testmod()

    import python_ta

    python_ta.check_all(config={
        'allowed-io': ['visualize_predictions'],
        'extra-imports': ['math', 'warnings', 'matplotlib.pyplot', 'numpy',
                          'pandas', 'sklearn.exceptions', 'sklearn.linear_model',
                          'sklearn.metrics', 'sklearn.model_selection',
                          'sklearn.preprocessing', 'plotly.express', 'sklearn.decomposition',
                          'sklearn.ensemble', 'sklearn.svm', 'reading_data'],
        'max-line-length': 100,
        'disable': ['R1705', 'C0200']
    })
